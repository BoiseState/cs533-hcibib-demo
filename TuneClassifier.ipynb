{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Example\n",
    "\n",
    "This notebook demonstrates hyperparameter tuning\n",
    "\n",
    "It uses the CHI Papers Data downloaded by the scripts in this project.  It trains various classifiers to predict whether a CHI paper is \"recent\" (written since 2005).\n",
    "\n",
    "All of these are going to optimize for **accuracy**, the metric returned by the `score` function on a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import our general PyData packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some SciKit Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the Bayesian optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want predictable randomness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(20201130)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have SciKit-Learn run some tasks in parallel.  Let's configure the (max) number of parallel tasks in one place, so you can easily adjust it based on your computer's capacity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NJOBS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We're going to load the CHI Papers data from the CSV file, output by the other notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.read_csv('chi-papers.csv', encoding='utf8')\n",
    "papers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's treat empty abstracts as empty strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['abstract'].fillna('', inplace=True)\n",
    "papers['title'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes we want all text - the title and the abstract.  We will join them with a space, so we don't fuse the last word of the title to the first word of the abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "papers['all_text'] = papers['title'] + ' ' + papers['abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to classify papers as *recent* if they're newer than 2005:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['IsRecent'] = papers['year'] > 2005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(papers, test_size=0.2, random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function for measuring accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(model, text='all_text'):\n",
    "    preds = model.predict(test[text])\n",
    "    print(classification_report(test['IsRecent'], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And look at the class distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(train['IsRecent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying New Papers\n",
    "\n",
    "Let's classify recent papers with k-NN on TF-IDF vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_knn = Pipeline([\n",
    "    ('vectorize', TfidfVectorizer(stop_words='english', lowercase=True, max_features=10000)),\n",
    "    ('class', KNeighborsClassifier(5))\n",
    "])\n",
    "base_knn.fit(train['all_text'], train['IsRecent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And measure it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure(base_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the Neighborhood\n",
    "\n",
    "Let's tune the neighborhood with a grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_knn = Pipeline([\n",
    "    ('vectorize', TfidfVectorizer(stop_words='english', lowercase=True, max_features=10000)),\n",
    "    ('class', GridSearchCV(KNeighborsClassifier(), param_grid={\n",
    "        'n_neighbors': [1, 2, 3, 5, 7, 10]\n",
    "    }, n_jobs=NJOBS))\n",
    "])\n",
    "tune_knn.fit(train['all_text'], train['IsRecent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did it pick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_knn.named_steps['class'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And measure it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure(tune_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Neighborhood\n",
    "\n",
    "Let's set up SVD-based neighborhood, and use random search to search both the latent feature count and the neighborhood size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_knn_inner = Pipeline([\n",
    "    ('latent', TruncatedSVD(random_state=rng)),\n",
    "    ('class', KNeighborsClassifier())\n",
    "])\n",
    "svd_knn = Pipeline([\n",
    "    ('vectorize', TfidfVectorizer(stop_words='english', lowercase=True)),\n",
    "    ('class', RandomizedSearchCV(svd_knn_inner, param_distributions={\n",
    "        'latent__n_components': stats.randint(1, 50),\n",
    "        'class__n_neighbors': stats.randint(1, 25)\n",
    "    }, n_iter=60, n_jobs=NJOBS, random_state=rng))\n",
    "])\n",
    "svd_knn.fit(train['all_text'], train['IsRecent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What parameters did we pick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_knn['class'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And measure it on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure(svd_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD with scikit-optimize\n",
    "\n",
    "Now let's cross-validate with SciKit-Optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_knn_inner = Pipeline([\n",
    "    ('latent', TruncatedSVD()),\n",
    "    ('class', KNeighborsClassifier())\n",
    "])\n",
    "svd_bayes_knn = Pipeline([\n",
    "    ('vectorize', TfidfVectorizer(stop_words='english', lowercase=True)),\n",
    "    ('class', BayesSearchCV(svd_knn_inner, {\n",
    "        'latent__n_components': (1, 50),\n",
    "        'class__n_neighbors': (1, 25)\n",
    "    }, n_jobs=NJOBS, random_state=rng))\n",
    "])\n",
    "svd_bayes_knn.fit(train['all_text'], train['IsRecent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What parameters did we pick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_bayes_knn['class'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And measure it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure(svd_bayes_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Let's give the Naive Bayes classifier a whirl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = Pipeline([\n",
    "    ('vectorize', TfidfVectorizer(stop_words='english', lowercase=True, max_features=10000)),\n",
    "    ('class', MultinomialNB())\n",
    "])\n",
    "nb.fit(train['all_text'], train['IsRecent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Accuracy\n",
    "\n",
    "What does our test accuracy look like for our various classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'kNN': base_knn,\n",
    "    'kNN-CV': tune_knn,\n",
    "    'kNN-SVD-Rand': svd_knn,\n",
    "    'kNN-SVD-Bayes': svd_bayes_knn,\n",
    "    'NB': nb\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = pd.DataFrame()\n",
    "for name, model in models.items():\n",
    "    all_preds[name] = model.predict(test['all_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = all_preds.apply(lambda ds: accuracy_score(test['IsRecent'], ds))\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.plot.bar()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
